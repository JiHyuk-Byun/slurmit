# myjob - SLURM Job Submission CLI

ë¡œì»¬ì—ì„œ ì›ê²© SLURM í´ëŸ¬ìŠ¤í„°ë¡œ jobì„ ì œì¶œí•˜ê³  ê´€ë¦¬í•˜ëŠ” CLI ë„êµ¬

## í”„ë¡œì íŠ¸ ê°œìš”

### ëª©í‘œ
- ë¡œì»¬ì—ì„œ ê°„ë‹¨í•œ ëª…ë ¹ìœ¼ë¡œ SLURM job ì œì¶œ
- Git ê¸°ë°˜ ì½”ë“œ ë™ê¸°í™” (ì»¤ë°‹ëœ ì½”ë“œë§Œ)
- Job ìƒíƒœ ëª¨ë‹ˆí„°ë§ ë° ë¡œê·¸ ì¡°íšŒ
- í´ëŸ¬ìŠ¤í„° ë…¸ë“œ/GPU ìƒíƒœ í™•ì¸

### ê¸°ìˆ  ìŠ¤íƒ
- **ì–¸ì–´**: Python 3.10+
- **CLI**: typer
- **SSH**: fabric
- **Config**: pydantic + pyyaml
- **ì½”ë“œ ë™ê¸°í™”**: git (clone)

---

## ì „ì²´ ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              LOCAL (Client)                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Phase 1 â”‚â”€â”€â”€â–¶â”‚ Phase 2 â”‚â”€â”€â”€â–¶â”‚ Phase 3 â”‚â”€â”€â”€â–¶â”‚ Phase 4 â”‚â”€â”€â”€â–¶â”‚ Phase 5 â”‚   â”‚
â”‚  â”‚  Parse  â”‚    â”‚   SSH   â”‚    â”‚   Git   â”‚    â”‚ Submit  â”‚    â”‚ Monitor â”‚   â”‚
â”‚  â”‚  Config â”‚    â”‚ Connect â”‚    â”‚  Clone  â”‚    â”‚   Job   â”‚    â”‚         â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â”‚ SSH (fabric)
                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                            REMOTE (SLURM Cluster)                            â”‚
â”‚                                                                              â”‚
â”‚   ~/.myjob/workspaces/{job_id}/                                             â”‚
â”‚   â”œâ”€â”€ (git cloned repo)                                                     â”‚
â”‚   â”œâ”€â”€ job.sh                                                                â”‚
â”‚   â”œâ”€â”€ env.sh                                                                â”‚
â”‚   â””â”€â”€ logs/                                                                 â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
myjob/
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ myjob/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ cli/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ main.py              # Entry point, typer app
â”‚   â”‚   â””â”€â”€ commands/
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ submit.py        # myjob submit
â”‚   â”‚       â”œâ”€â”€ status.py        # myjob status
â”‚   â”‚       â”œâ”€â”€ logs.py          # myjob logs
â”‚   â”‚       â”œâ”€â”€ cancel.py        # myjob cancel
â”‚   â”‚       â”œâ”€â”€ list.py          # myjob list
â”‚   â”‚       â””â”€â”€ nodes.py         # myjob nodes
â”‚   â”‚
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ config.py            # Config loading & validation
â”‚   â”‚   â”œâ”€â”€ models.py            # Pydantic models
â”‚   â”‚   â””â”€â”€ job_id.py            # Local job ID generation
â”‚   â”‚
â”‚   â”œâ”€â”€ transport/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ ssh.py               # SSH connection (fabric)
â”‚   â”‚   â””â”€â”€ git_sync.py          # Git clone to remote
â”‚   â”‚
â”‚   â”œâ”€â”€ backend/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ slurm.py             # SLURM script generation & submission
â”‚   â”‚
â”‚   â”œâ”€â”€ monitor/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ status.py            # Job status (squeue, sacct)
â”‚   â”‚   â”œâ”€â”€ logs.py              # Log viewing
â”‚   â”‚   â””â”€â”€ nodes.py             # Node/GPU status
â”‚   â”‚
â”‚   â””â”€â”€ storage/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ job_store.py         # Local job history (JSON files)
â”‚
â””â”€â”€ tests/
    â””â”€â”€ ...
```

---

## ì‚¬ìš©ì íŒŒì¼ êµ¬ì¡°

```
user-project/
â”œâ”€â”€ myjob.yaml        # ë©”ì¸ ì„¤ì • (gitì— ì»¤ë°‹ OK)
â”œâ”€â”€ secret.yaml       # ë¯¼ê° ì •ë³´ (.gitignoreì— ì¶”ê°€)
â”œâ”€â”€ .gitignore
â””â”€â”€ src/
    â””â”€â”€ ...
```

---

## Phase 1: ì…ë ¥ ì²˜ë¦¬ ë° ê²€ì¦

### ì„¤ì • ìš°ì„ ìˆœìœ„

```
CLI args > myjob.yaml > secret.yaml > defaults
```

### Config ìŠ¤í‚¤ë§ˆ (Pydantic)

```python
# myjob/core/models.py

from pydantic import BaseModel, Field, validator
from typing import Optional, List, Dict
from pathlib import Path

class ConnectionConfig(BaseModel):
    host: str
    port: int = 22
    user: Optional[str] = None  # Noneì´ë©´ í˜„ì¬ ì‚¬ìš©ì
    key_path: Optional[str] = None  # Noneì´ë©´ ê¸°ë³¸ SSH í‚¤

class SlurmConfig(BaseModel):
    """SLURM ì „ìš© ì˜µì…˜"""
    partition: Optional[str] = None
    account: Optional[str] = None
    qos: Optional[str] = None
    constraint: Optional[str] = None  # ë…¸ë“œ ì œì•½ (e.g., "a100")
    reservation: Optional[str] = None
    
    # ë°°ì—´ ì‘ì—…
    array: Optional[str] = None  # "1-100", "1-100%10"
    
    # ì˜ì¡´ì„±
    dependency: Optional[str] = None  # "afterok:12345"
    
    # ì¶”ê°€ sbatch ì˜µì…˜ (escape hatch)
    extra_args: List[str] = []  # ["--exclusive", "--requeue"]

class ResourceConfig(BaseModel):
    cpus: int = Field(default=1, ge=1)
    memory: str = "4G"
    gpus: int = Field(default=0, ge=0)
    gpu_type: Optional[str] = None  # "a100", "v100"
    nodes: int = Field(default=1, ge=1)
    time: str = "1:00:00"  # ê¸°ë³¸ 1ì‹œê°„
    
    @validator('memory')
    def validate_memory(cls, v):
        """4G, 4096M, 4096 ë“± íŒŒì‹±"""
        import re
        match = re.match(r'^(\d+)([GMK]?)$', v.upper())
        if not match:
            raise ValueError(f"Invalid memory format: {v}")
        return v

class GitConfig(BaseModel):
    repo: Optional[str] = None  # Noneì´ë©´ ìë™ ê°ì§€
    branch: Optional[str] = None  # Noneì´ë©´ í˜„ì¬ ë¸Œëœì¹˜
    commit: Optional[str] = None  # Noneì´ë©´ HEAD

class ExecutionConfig(BaseModel):
    command: Optional[str] = None
    script: Optional[str] = None
    working_dir: str = "."
    env: Dict[str, str] = {}
    
    # ì‹¤í–‰ ì „/í›„ í›…
    setup: Optional[str] = None  # "module load cuda/11.8"
    teardown: Optional[str] = None

class OutputConfig(BaseModel):
    stdout: str = "logs/stdout_%j.log"
    stderr: str = "logs/stderr_%j.log"
    fetch: List[str] = []  # ì™„ë£Œ í›„ ê°€ì ¸ì˜¬ íŒŒì¼
    cleanup: bool = False  # ì™„ë£Œ í›„ ì›ê²© workspace ì‚­ì œ

class JobConfig(BaseModel):
    """ìµœì¢… í†µí•© ì„¤ì •"""
    name: Optional[str] = None
    
    connection: ConnectionConfig
    slurm: SlurmConfig = SlurmConfig()
    resources: ResourceConfig = ResourceConfig()
    git: GitConfig = GitConfig()
    execution: ExecutionConfig
    output: OutputConfig = OutputConfig()
    
    tags: List[str] = []
```

### myjob.yaml ì˜ˆì‹œ

```yaml
name: gpt-training

connection:
  host: cluster.example.com
  user: myuser

slurm:
  partition: gpu
  account: research-lab
  qos: normal

resources:
  gpus: 2
  gpu_type: a100
  cpus: 8
  memory: 32G
  time: "24:00:00"

execution:
  command: python train.py --epochs 100
  setup: |
    module load cuda/11.8
    module load anaconda3
    conda activate myenv

output:
  fetch:
    - outputs/
    - logs/
```

### secret.yaml ì˜ˆì‹œ

```yaml
# .gitignoreì— ì¶”ê°€í•  ê²ƒ
env:
  WANDB_API_KEY: "abc123..."
  HF_TOKEN: "hf_xxxxx..."

# SSH í‚¤ (ì„ íƒ)
connection:
  key_path: ~/.ssh/my_private_key
```

### Config ë¡œë“œ ë¡œì§

```python
# myjob/core/config.py

class ConfigLoader:
    def load(self, config_file: Optional[str], cli_args: dict) -> JobConfig:
        """
        1. myjob.yaml ë¡œë“œ
        2. secret.yaml ë³‘í•© (ê°™ì€ ë””ë ‰í† ë¦¬)
        3. ê¸€ë¡œë²Œ secret (~/.myjob/secret.yaml) ë³‘í•©
        4. CLI args ì˜¤ë²„ë¼ì´ë“œ
        5. Pydantic ê²€ì¦
        """
        merged = {}
        
        # 1. myjob.yaml
        config_path = config_file or self._find_config()
        if config_path:
            merged = self._load_yaml(config_path)
        
        # 2. secret.yaml (í”„ë¡œì íŠ¸)
        secret_path = Path(config_path).parent / "secret.yaml"
        if secret_path.exists():
            secrets = self._load_yaml(secret_path)
            merged = self._deep_merge(merged, secrets)
        
        # 3. secret.yaml (ê¸€ë¡œë²Œ)
        global_secret = Path.home() / ".myjob" / "secret.yaml"
        if global_secret.exists():
            global_secrets = self._load_yaml(global_secret)
            merged = self._deep_merge(global_secrets, merged)
        
        # 4. CLI args
        merged = self._deep_merge(merged, cli_args)
        
        # 5. ê²€ì¦
        return JobConfig(**merged)
```

---

## Phase 2: SSH ì—°ê²°

### êµ¬í˜„

```python
# myjob/transport/ssh.py

from fabric import Connection
from dataclasses import dataclass

@dataclass
class RemoteInfo:
    slurm_version: str
    home_dir: str
    workspace_base: str
    available_partitions: list[str]

class SSHClient:
    def __init__(self, config: ConnectionConfig):
        self.config = config
        self.conn: Connection = None
        self.remote_info: RemoteInfo = None
    
    def connect(self) -> None:
        """SSH ì—°ê²° ìˆ˜ë¦½"""
        connect_kwargs = {}
        if self.config.key_path:
            connect_kwargs["key_filename"] = self.config.key_path
        
        self.conn = Connection(
            host=self.config.host,
            port=self.config.port,
            user=self.config.user,
            connect_kwargs=connect_kwargs,
            connect_timeout=30,
        )
        
        # ì—°ê²° í…ŒìŠ¤íŠ¸
        self.conn.run("echo 'connected'", hide=True)
    
    def check_environment(self) -> RemoteInfo:
        """ì›ê²© í™˜ê²½ í™•ì¸"""
        # SLURM ë²„ì „
        result = self.conn.run("sinfo --version", hide=True, warn=True)
        if result.failed:
            raise EnvironmentError("SLURM not found on remote server")
        slurm_version = result.stdout.strip()
        
        # í™ˆ ë””ë ‰í† ë¦¬
        home_dir = self.conn.run("echo $HOME", hide=True).stdout.strip()
        
        # íŒŒí‹°ì…˜ ëª©ë¡
        result = self.conn.run("sinfo -h -o '%P'", hide=True)
        partitions = [p.strip().rstrip('*') for p in result.stdout.splitlines()]
        
        self.remote_info = RemoteInfo(
            slurm_version=slurm_version,
            home_dir=home_dir,
            workspace_base=f"{home_dir}/.myjob/workspaces",
            available_partitions=partitions,
        )
        return self.remote_info
    
    def setup_workspace(self, job_id: str) -> str:
        """ì›ê²© workspace ìƒì„±"""
        workspace = f"{self.remote_info.workspace_base}/{job_id}"
        self.conn.run(f"mkdir -p {workspace}/logs", hide=True)
        return workspace
    
    def run(self, command: str, **kwargs):
        return self.conn.run(command, **kwargs)
    
    def close(self):
        if self.conn:
            self.conn.close()
```

### ì—ëŸ¬ ì²˜ë¦¬
- ì—°ê²° ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„ 3íšŒ
- íƒ€ì„ì•„ì›ƒ 30ì´ˆ

---

## Phase 3: Git ë™ê¸°í™”

### ì •ì±…
- **ì»¤ë°‹ëœ ì½”ë“œë§Œ ì‚¬ìš©** (uncommitted ë³€ê²½ì‚¬í•­ ë¬´ì‹œ)
- uncommitted ë³€ê²½ì‚¬í•­ì´ ìˆìœ¼ë©´ ê²½ê³  ì¶œë ¥ (--forceë¡œ ë¬´ì‹œ ê°€ëŠ¥)

### êµ¬í˜„

```python
# myjob/transport/git_sync.py

import subprocess
from dataclasses import dataclass

@dataclass
class GitInfo:
    repo_url: str
    branch: str
    commit_hash: str
    commit_message: str

class GitSyncer:
    def get_local_git_info(self) -> GitInfo:
        """ë¡œì»¬ git ì •ë³´ ì¶”ì¶œ"""
        repo_url = self._run_git("remote", "get-url", "origin")
        branch = self._run_git("branch", "--show-current")
        commit_hash = self._run_git("rev-parse", "HEAD")
        commit_message = self._run_git("log", "-1", "--format=%s")
        
        return GitInfo(repo_url, branch, commit_hash, commit_message)
    
    def check_clean(self) -> bool:
        """uncommitted ë³€ê²½ì‚¬í•­ í™•ì¸"""
        result = subprocess.run(
            ["git", "status", "--porcelain"],
            capture_output=True, text=True
        )
        return len(result.stdout.strip()) == 0
    
    def sync_to_remote(self, ssh_client: SSHClient, workspace: str, git_info: GitInfo):
        """ì›ê²©ì— git clone"""
        commands = f"""
        git clone --branch {git_info.branch} --depth 1 {git_info.repo_url} {workspace}
        cd {workspace}
        git checkout {git_info.commit_hash}
        """
        ssh_client.run(commands, hide=True)
    
    def _run_git(self, *args) -> str:
        result = subprocess.run(
            ["git"] + list(args),
            capture_output=True, text=True, check=True
        )
        return result.stdout.strip()
```

---

## Phase 4: Job ì œì¶œ

### sbatch ìŠ¤í¬ë¦½íŠ¸ ìƒì„±

```python
# myjob/backend/slurm.py

class SlurmBackend:
    def generate_script(self, config: JobConfig, workspace: str) -> str:
        lines = ["#!/bin/bash"]
        
        # Job ì´ë¦„
        if config.name:
            lines.append(f"#SBATCH --job-name={config.name}")
        
        # ë¦¬ì†ŒìŠ¤
        lines.append(f"#SBATCH --nodes={config.resources.nodes}")
        lines.append(f"#SBATCH --cpus-per-task={config.resources.cpus}")
        lines.append(f"#SBATCH --mem={config.resources.memory}")
        lines.append(f"#SBATCH --time={config.resources.time}")
        
        # GPU
        if config.resources.gpus > 0:
            gpu_spec = f"gpu:{config.resources.gpus}"
            if config.resources.gpu_type:
                gpu_spec = f"gpu:{config.resources.gpu_type}:{config.resources.gpus}"
            lines.append(f"#SBATCH --gres={gpu_spec}")
        
        # SLURM ì˜µì…˜
        if config.slurm.partition:
            lines.append(f"#SBATCH --partition={config.slurm.partition}")
        if config.slurm.account:
            lines.append(f"#SBATCH --account={config.slurm.account}")
        if config.slurm.qos:
            lines.append(f"#SBATCH --qos={config.slurm.qos}")
        if config.slurm.constraint:
            lines.append(f"#SBATCH --constraint={config.slurm.constraint}")
        if config.slurm.array:
            lines.append(f"#SBATCH --array={config.slurm.array}")
        if config.slurm.dependency:
            lines.append(f"#SBATCH --dependency={config.slurm.dependency}")
        
        # ë¡œê·¸ ê²½ë¡œ
        lines.append(f"#SBATCH --output={workspace}/logs/stdout_%j.log")
        lines.append(f"#SBATCH --error={workspace}/logs/stderr_%j.log")
        
        # ì¶”ê°€ ì˜µì…˜
        for arg in config.slurm.extra_args:
            lines.append(f"#SBATCH {arg}")
        
        lines.append("")
        lines.append(f"cd {workspace}")
        lines.append("source ./env.sh")
        lines.append("")
        
        # Setup
        if config.execution.setup:
            lines.append("# Setup")
            lines.append(config.execution.setup)
            lines.append("")
        
        # ë©”ì¸ ëª…ë ¹
        lines.append("# Main command")
        if config.execution.script:
            lines.append(f"bash {config.execution.script}")
        else:
            lines.append(config.execution.command)
        
        # Teardown
        if config.execution.teardown:
            lines.append("")
            lines.append("# Teardown")
            lines.append(config.execution.teardown)
        
        return "\n".join(lines)
    
    def generate_env_script(self, env: dict) -> str:
        """env.sh ìƒì„±"""
        lines = ["#!/bin/bash"]
        for key, value in env.items():
            lines.append(f'export {key}="{value}"')
        return "\n".join(lines)
    
    def submit(self, ssh_client: SSHClient, workspace: str) -> str:
        """sbatch ì‹¤í–‰ ë° job ID ë°˜í™˜"""
        result = ssh_client.run(
            f"sbatch {workspace}/job.sh",
            hide=True
        )
        # "Submitted batch job 12345678" íŒŒì‹±
        output = result.stdout.strip()
        slurm_job_id = output.split()[-1]
        return slurm_job_id
```

### ë¡œì»¬ Job ì €ì¥

```python
# myjob/storage/job_store.py

import json
from pathlib import Path
from datetime import datetime
from dataclasses import dataclass, asdict

@dataclass
class JobRecord:
    local_id: str           # ë¡œì»¬ ì¶”ì  ID (e.g., "a1b2c3")
    slurm_job_id: str       # SLURM job ID (e.g., "12345678")
    name: str
    host: str
    workspace: str
    git_repo: str
    git_branch: str
    git_commit: str
    status: str             # PENDING, RUNNING, COMPLETED, FAILED, CANCELLED
    submitted_at: str       # ISO format
    config: dict            # ì›ë³¸ config

class JobStore:
    def __init__(self):
        self.store_path = Path.home() / ".myjob" / "jobs"
        self.store_path.mkdir(parents=True, exist_ok=True)
    
    def save(self, job: JobRecord):
        path = self.store_path / f"{job.local_id}.json"
        path.write_text(json.dumps(asdict(job), indent=2))
    
    def get(self, local_id: str) -> JobRecord:
        path = self.store_path / f"{local_id}.json"
        data = json.loads(path.read_text())
        return JobRecord(**data)
    
    def list_recent(self, limit: int = 20) -> list[JobRecord]:
        jobs = []
        for path in sorted(self.store_path.glob("*.json"), reverse=True)[:limit]:
            data = json.loads(path.read_text())
            jobs.append(JobRecord(**data))
        return jobs
    
    def update_status(self, local_id: str, status: str):
        job = self.get(local_id)
        job.status = status
        self.save(job)
```

---

## Phase 5: ëª¨ë‹ˆí„°ë§

### 5.1 Job ìƒíƒœ ì¡°íšŒ

```python
# myjob/monitor/status.py

@dataclass
class JobStatus:
    state: str              # PENDING, RUNNING, COMPLETED, FAILED, CANCELLED, TIMEOUT
    elapsed_time: str
    node: Optional[str] = None
    reason: Optional[str] = None
    exit_code: Optional[str] = None

class JobMonitor:
    def __init__(self, ssh_client: SSHClient, job_store: JobStore):
        self.ssh = ssh_client
        self.store = job_store
    
    def get_status(self, local_id: str) -> JobStatus:
        job = self.store.get(local_id)
        
        # squeueë¡œ í™•ì¸ (íì— ìˆëŠ” job)
        result = self.ssh.run(
            f"squeue -j {job.slurm_job_id} -h -o '%T|%M|%N|%R'",
            warn=True, hide=True
        )
        
        if result.ok and result.stdout.strip():
            parts = result.stdout.strip().split('|')
            return JobStatus(
                state=parts[0],
                elapsed_time=parts[1],
                node=parts[2] if parts[2] else None,
                reason=parts[3] if len(parts) > 3 else None,
            )
        
        # sacctë¡œ í™•ì¸ (ì™„ë£Œëœ job)
        result = self.ssh.run(
            f"sacct -j {job.slurm_job_id} -n -o State,ExitCode,Elapsed -P",
            hide=True
        )
        
        if result.stdout.strip():
            lines = result.stdout.strip().split('\n')
            parts = lines[0].split('|')
            return JobStatus(
                state=parts[0],
                exit_code=parts[1],
                elapsed_time=parts[2],
            )
        
        return JobStatus(state="UNKNOWN", elapsed_time="")
```

### 5.2 ë¡œê·¸ ì¡°íšŒ

```python
# myjob/monitor/logs.py

class LogViewer:
    def __init__(self, ssh_client: SSHClient, job_store: JobStore):
        self.ssh = ssh_client
        self.store = job_store
    
    def get_logs(self, local_id: str, stderr: bool = False, lines: int = 100) -> str:
        job = self.store.get(local_id)
        log_type = "stderr" if stderr else "stdout"
        log_path = f"{job.workspace}/logs/{log_type}_{job.slurm_job_id}.log"
        
        result = self.ssh.run(f"tail -n {lines} {log_path}", hide=True, warn=True)
        return result.stdout if result.ok else f"Log not found: {log_path}"
    
    def follow_logs(self, local_id: str, stderr: bool = False):
        job = self.store.get(local_id)
        log_type = "stderr" if stderr else "stdout"
        log_path = f"{job.workspace}/logs/{log_type}_{job.slurm_job_id}.log"
        
        self.ssh.run(f"tail -f {log_path}", pty=True)
```

### 5.3 ì‘ì—… ì·¨ì†Œ

```python
# myjob/monitor/cancel.py

class JobCanceller:
    def __init__(self, ssh_client: SSHClient, job_store: JobStore):
        self.ssh = ssh_client
        self.store = job_store
    
    def cancel(self, local_id: str, force: bool = False) -> bool:
        job = self.store.get(local_id)
        
        cmd = f"scancel {job.slurm_job_id}"
        if force:
            cmd = f"scancel --signal=KILL {job.slurm_job_id}"
        
        result = self.ssh.run(cmd, warn=True, hide=True)
        
        if result.ok:
            self.store.update_status(local_id, "CANCELLED")
            return True
        return False
```

### 5.4 ë…¸ë“œ/GPU ìƒíƒœ

```python
# myjob/monitor/nodes.py

@dataclass
class GPUInfo:
    gpu_type: str       # a100, v100 ë“±
    total: int
    used: int
    free: int

@dataclass
class NodeInfo:
    name: str
    state: str          # idle, mixed, allocated, down
    partition: str
    cpus_total: int
    cpus_used: int
    memory_total: str
    gpu: Optional[GPUInfo]

class NodeMonitor:
    def __init__(self, ssh_client: SSHClient):
        self.ssh = ssh_client
    
    def get_nodes(self, partition: str = None) -> list[NodeInfo]:
        # 1. sinfoë¡œ ê¸°ë³¸ ì •ë³´
        cmd = "sinfo -N -h -o '%N|%P|%T|%C|%m'"
        if partition:
            cmd += f" -p {partition}"
        
        result = self.ssh.run(cmd, hide=True)
        nodes = {}
        
        for line in result.stdout.strip().split('\n'):
            if not line:
                continue
            name, part, state, cpus, mem = line.split('|')
            cpu_parts = cpus.split('/')
            
            nodes[name] = NodeInfo(
                name=name,
                partition=part.rstrip('*'),
                state=state,
                cpus_total=int(cpu_parts[3]),
                cpus_used=int(cpu_parts[0]),
                memory_total=mem,
                gpu=None,
            )
        
        # 2. scontrol show nodeë¡œ GPU ì •ë³´
        for node_name in nodes:
            nodes[node_name].gpu = self._get_gpu_info(node_name)
        
        return list(nodes.values())
    
    def _get_gpu_info(self, node_name: str) -> Optional[GPUInfo]:
        result = self.ssh.run(f"scontrol show node {node_name}", hide=True, warn=True)
        if not result.ok:
            return None
        
        output = result.stdout
        
        # Gres=gpu:a100:4 íŒŒì‹±
        gres_total = self._parse_field(output, "Gres")
        gres_used = self._parse_field(output, "GresUsed")
        
        if not gres_total or gres_total == "(null)":
            return None
        
        gpu_type, total = self._parse_gres(gres_total)
        _, used = self._parse_gres(gres_used) if gres_used else (None, 0)
        
        return GPUInfo(
            gpu_type=gpu_type,
            total=total,
            used=used,
            free=total - used,
        )
    
    def _parse_field(self, output: str, field: str) -> Optional[str]:
        for line in output.split('\n'):
            for part in line.split():
                if part.startswith(f"{field}="):
                    return part.split('=', 1)[1]
        return None
    
    def _parse_gres(self, gres_str: str) -> tuple[str, int]:
        """gpu:a100:4 â†’ ("a100", 4)"""
        if not gres_str or gres_str == "(null)":
            return ("unknown", 0)
        
        gres_str = gres_str.split('(')[0]  # (IDX:...) ì œê±°
        parts = gres_str.split(':')
        
        if len(parts) == 3:
            return (parts[1], int(parts[2]))
        elif len(parts) == 2:
            return ("gpu", int(parts[1]))
        
        return ("unknown", 0)
```

---

## CLI ì¸í„°í˜ì´ìŠ¤

### ëª…ë ¹ì–´ ëª©ë¡

```bash
myjob submit                 # Job ì œì¶œ
myjob status <job_id>        # ìƒíƒœ í™•ì¸
myjob logs <job_id>          # ë¡œê·¸ ì¡°íšŒ
myjob cancel <job_id>        # ì‘ì—… ì·¨ì†Œ
myjob list                   # ì œì¶œí•œ ì‘ì—… ëª©ë¡
myjob nodes                  # í´ëŸ¬ìŠ¤í„° ë…¸ë“œ/GPU ìƒíƒœ
myjob init                   # ì„¤ì • íŒŒì¼ ìƒì„± (interactive)
```

### CLI êµ¬í˜„

```python
# myjob/cli/main.py

import typer
from typing import Optional, List

app = typer.Typer(name="myjob", help="Submit jobs to SLURM clusters")

@app.command()
def submit(
    config: Optional[str] = typer.Option(None, "-c", "--config"),
    host: Optional[str] = typer.Option(None, "-H", "--host"),
    partition: Optional[str] = typer.Option(None, "-p", "--partition"),
    account: Optional[str] = typer.Option(None, "-A", "--account"),
    gpus: Optional[int] = typer.Option(None, "-g", "--gpus"),
    cpus: Optional[int] = typer.Option(None, "-n", "--cpus"),
    memory: Optional[str] = typer.Option(None, "-m", "--memory"),
    time: Optional[str] = typer.Option(None, "-t", "--time"),
    name: Optional[str] = typer.Option(None, "--name"),
    force: bool = typer.Option(False, "--force", help="Ignore uncommitted changes"),
    wait: bool = typer.Option(False, "-w", "--wait", help="Wait for completion"),
    follow: bool = typer.Option(False, "-f", "--follow", help="Follow logs"),
    command: Optional[List[str]] = typer.Argument(None),
):
    """Submit a job to SLURM cluster"""
    pass

@app.command()
def status(
    job_id: str = typer.Argument(...),
    watch: bool = typer.Option(False, "-w", "--watch"),
):
    """Check job status"""
    pass

@app.command()
def logs(
    job_id: str = typer.Argument(...),
    follow: bool = typer.Option(False, "-f", "--follow"),
    stderr: bool = typer.Option(False, "-e", "--stderr"),
    lines: int = typer.Option(100, "-n", "--lines"),
):
    """View job logs"""
    pass

@app.command()
def cancel(
    job_id: str = typer.Argument(...),
    force: bool = typer.Option(False, "--force"),
):
    """Cancel a job"""
    pass

@app.command(name="list")
def list_jobs(
    all: bool = typer.Option(False, "-a", "--all"),
    limit: int = typer.Option(20, "-n", "--limit"),
):
    """List submitted jobs"""
    pass

@app.command()
def nodes(
    partition: Optional[str] = typer.Option(None, "-p", "--partition"),
    verbose: bool = typer.Option(False, "-v", "--verbose"),
):
    """Show cluster node/GPU status"""
    pass

@app.command()
def init():
    """Initialize config file interactively"""
    pass

if __name__ == "__main__":
    app()
```

---

## ì‚¬ìš© ì˜ˆì‹œ

### Job ì œì¶œ

```bash
# Config íŒŒì¼ ì‚¬ìš©
$ myjob submit -c myjob.yaml

# CLI ì˜¤ë²„ë¼ì´ë“œ
$ myjob submit -c myjob.yaml --gpus 4 --time 48:00:00

# Config ì—†ì´
$ myjob submit -H cluster.example.com -p gpu -g 2 -m 16G -- python train.py
```

### ì¶œë ¥ ì˜ˆì‹œ

```
[1/5] ğŸ” Validating configuration...
      âœ“ Config valid

[2/5] ğŸ”Œ Connecting to cluster.example.com...
      âœ“ Connected (SLURM 23.02)

[3/5] ğŸ“¦ Syncing code via git...
      âš ï¸  Warning: You have uncommitted changes (use --force to ignore)

$ git commit -am "experiment" && git push

$ myjob submit -c myjob.yaml

[3/5] ğŸ“¦ Syncing code via git...
      Repository: git@github.com:user/project.git
      Branch: main
      Commit: a1b2c3d
      âœ“ Cloned to remote workspace

[4/5] ğŸš€ Submitting job...
      âœ“ Job submitted: SLURM_JOB_ID=12345678

[5/5] ğŸ“‹ Job registered
      Local ID: a1b2c3

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Job submitted successfully!

Status:  myjob status a1b2c3
Logs:    myjob logs a1b2c3 -f
Cancel:  myjob cancel a1b2c3
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

### ìƒíƒœ í™•ì¸

```bash
$ myjob status a1b2c3

Job: a1b2c3 (SLURM ID: 12345678)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Status:    RUNNING
Elapsed:   02:34:15
Node:      gpu-node-03
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Git:       main @ a1b2c3d
Command:   python train.py --epochs 100
```

### ë…¸ë“œ ìƒíƒœ

```bash
$ myjob nodes

CLUSTER STATUS
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
NODE          STATE      PARTITION   CPU (used/total)   GPU (free/total)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
gpu-node-01   mixed      gpu         24/64              2/4 (a100)
gpu-node-02   allocated  gpu         64/64              0/4 (a100)
gpu-node-03   idle       gpu         0/64               4/4 (a100)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

SUMMARY
  Total GPUs: 12 (A100: 12)
  Free GPUs:  6 (A100: 6)
```

---

## êµ¬í˜„ ìš°ì„ ìˆœìœ„

### Phase 1: MVP (ìµœì†Œ ê¸°ëŠ¥)
1. Config ë¡œë“œ (myjob.yaml, secret.yaml)
2. SSH ì—°ê²° (fabric)
3. Git clone
4. sbatch ìŠ¤í¬ë¦½íŠ¸ ìƒì„± ë° ì œì¶œ
5. ê¸°ë³¸ status, logs ì¡°íšŒ

### Phase 2: ì™„ì„±ë„
6. cancel ê¸°ëŠ¥
7. list ê¸°ëŠ¥
8. nodes ê¸°ëŠ¥
9. ì—ëŸ¬ ì²˜ë¦¬ ê°œì„ 
10. init ëª…ë ¹ì–´

### Phase 3: í¸ì˜ ê¸°ëŠ¥
11. --wait, --follow ì˜µì…˜
12. ì´ìœ ì¶œë ¥ (rich ë¼ì´ë¸ŒëŸ¬ë¦¬)
13. Tab completion
